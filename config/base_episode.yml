# timezone, it will use to get run_id
timezone: Asia/Ho_Chi_Minh

model:
  name: baseline
  # backbone support: [osnet, resnet50, resnet101, resnet50_nl, resnet101_nl, resnet50_ibn_a,
  #                    resnet101_ibn_a, resnet50_ibn_a_nl, resnet101_ibn_a_nl]
  # nl: https://arxiv.org/abs/1711.07971
  # ibn_a: https://github.com/XingangPan/IBN-Net
  backbone: resnet50
  pretrained: True
  # global pooling support: [avg_pooling, gem_pooling]
  pooling: gem_pooling
  # head support: [BNHead, ReductionHead]
  head: BNHead
  # bn_where before or after of liner layer
  bn_where: after
  # set bias of batch norm in head layer
  batch_norm_bias: True
  # using tqdm process bar when download pretrained model
  use_tqdm: False

data:
  # folder will save data
  data_dir: /home/ubuntu/Documents/datasets
  # name of dataset: [peta, pa100k, ppe, ppe_two]
  name: peta
  # automatic download and extract model from my drive
  download: True
  extract: True
  # using tqdm process bar when download and extract dataset file
  use_tqdm: False
  train:
    num_attribute: 4
    num_instance: 8
    num_iterator: 500
    selected_ratio: 0.5
  val:
    num_attribute: 4
    num_instance: 8
    num_iterator: 100
    selected_ratio: 0.5

# optimizer support: [adam, sgd]
optimizer:
  name: adam
  lr: 0.00035
  specified_lr:
    enable: False
    lr: 0.1
    layers: [head]
  weight_decay: 0.0005
  adam_beta1: 0.9
  adam_beta2: 0.99
  momentum: 0.9
  sgd_dampening: 0
  sgd_nesterov: False

# loss support: [Singular_BCE]
loss:
  name: Singular_BCE

# freeze <layers> at first <epochs>.
freeze:
  enable: False
  layers: [backbone]
  epochs: 10

# learning rate scheduler. if you not config manual parameters, it will get from default, corresponding name
lr_scheduler:
  enable: True
  name: MultiStepLR
  default:
    WarmupMultiStepLR:
      steps: [40, 90]
      gamma: 0.1
      warmup_factor: 0.01
      warmup_iters: 10
      warmup_method: linear
    ReduceLROnPlateau:
      factor: 0.1
      patience: 10
      min_lr: 0.0000001
    MultiStepLR:
      steps: [40, 70]
      gamma: 0.1
    WarmupCosineAnnealingLR:
      max_iters: 120
      delay_iters: 30
      eta_min_lr: 0.00000077
      warmup_factor: 0.01
      warmup_iters: 10
      warmup_method: linear

# clipping gradient
clip_grad_norm_:
  enable: False
  max_norm: 10.0

# if colab argument is true -> trainer_colab else -> trainer
# train on local
# use_tqdm: use tqdm process bar when training.
trainer:
  # 0 if using cpu. > 0 if using gpu
  n_gpu: 0
  # maximum epoch
  epochs: 120
  # saved log folder
  checkpoint_dir: saved/checkpoints
  log_dir: saved/logs
  output_dir: saved/outputs
  use_tqdm: True

# train on colab
# log folder will copy to log_dir_saved at end of epoch
# use_tqdm: use tqdm process bar when training.
trainer_colab:
  n_gpu: 1
  epochs: 120
  checkpoint_dir: /content/drive/Shared drives/REID/HIEN/Models/OSNet_Person_Attribute_Refactor/checkpoints
  log_dir: saved/logs
  log_dir_saved: /content/drive/Shared drives/REID/HIEN/Models/OSNet_Person_Attribute_Refactor/logs
  output_dir: /content/drive/Shared drives/REID/HIEN/Models/OSNet_Person_Attribute_Refactor/outputs
  use_tqdm: False