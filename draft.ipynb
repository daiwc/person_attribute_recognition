{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597057736043",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from easydict import EasyDict\n",
    "\n",
    "from models import build_model\n",
    "from data import DataManger_Epoch, DataManger_Episode\n",
    "from logger import setup_logging\n",
    "from utils import read_config, rmdir, summary, array_interweave, COLOR\n",
    "from evaluators import recognition_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_test(logger_func, attribute_name: list, weight, result_label, result_instance):\n",
    "    r\"\"\" log test from result\n",
    "    \"\"\"\n",
    "    logger_func('instance-based metrics:')\n",
    "    logger_func('accuracy: %0.4f' % result_instance.accuracy)\n",
    "    logger_func('precision: %0.4f' % result_instance.precision)\n",
    "    logger_func('recall: %0.4f' % result_instance.recall)\n",
    "    logger_func('f1_score: %0.4f' % result_instance.f1_score)\n",
    "\n",
    "    logger_func('class-based metrics:')\n",
    "    result = np.stack([result_label.accuracy, result_label.mean_accuracy, result_label.precision, result_label.recall, result_label.f1_score], axis=0)\n",
    "    result = np.around(result*100, 2)\n",
    "    result = result.transpose()\n",
    "    row_format =\"{:>20}\" + \"{:>10}\"*6\n",
    "    \n",
    "    logger_func(row_format.format('attribute', 'weight', 'accuracy', 'mA', 'precision', 'recall', 'f1_score'))\n",
    "    \n",
    "    logger_func(row_format.format(*['-']*7))\n",
    "    \n",
    "    for i in range(len(attribute_name)):\n",
    "        logger_func(row_format.format(attribute_name[i], np.around(weight[i]*100, 2), *result[i].tolist()))\n",
    "\n",
    "    logger_func(row_format.format(*['-']*7))\n",
    "    \n",
    "    logger_func(row_format.format(\n",
    "        'mean',\n",
    "        '-',\n",
    "        round(np.mean(result_label.accuracy)*100, 2),\n",
    "        round(np.mean(result_label.mean_accuracy)*100, 2),\n",
    "        round(np.mean(result_label.precision)*100, 2),\n",
    "        round(np.mean(result_label.recall)*100, 2),\n",
    "        round(np.mean(result_label.f1_score)*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_class_based(logger_func, attribute_name, weight, result_label1, result_label2, color=COLOR.BOLD):\n",
    "    \n",
    "    logger_func('class-based metrics:')\n",
    "    result1 = np.stack([result_label1.accuracy, result_label1.mean_accuracy, result_label1.precision, result_label1.recall, result_label1.f1_score], axis=0)\n",
    "    result1 = np.around(result1*100, 2)\n",
    "    result1 = result1.transpose()\n",
    "\n",
    "    result2 = np.stack([result_label2.accuracy, result_label2.mean_accuracy, result_label2.precision, result_label2.recall, result_label2.f1_score], axis=0)\n",
    "    result2 = np.around(result2*100, 2)\n",
    "    result2 = result2.transpose()\n",
    "\n",
    "    row_format = \"{:>20}{:>12}{:>15}{:>12}{:>20}{:>14}{:>18}\"\n",
    "    logger_func(row_format.format('attribute', 'weight', 'accuracy', 'mA', 'precision', 'recall', 'f1_score'))\n",
    "    \n",
    "    logger_func(row_format.format(*['-']*7))\n",
    "\n",
    "    for i in range(len(attribute_name)):\n",
    "        row_format = \"{:>20}\" + \"{:>12}\"\n",
    "        for j in range(5):\n",
    "            if result1[i][j] > result2[i][j]:\n",
    "                row_format += color + \"{:>10}\" + COLOR.END +\"|{:>5}\"\n",
    "            elif result1[i][j] < result2[i][j]:\n",
    "                row_format += \"{:>10}|\" + color + \"{:>5}\" + COLOR.END\n",
    "            else:\n",
    "                row_format += color + \"{:>10}\" + COLOR.END + \"|\" + color + \"{:>5}\" + COLOR.END\n",
    "        logger_func(row_format.format(attribute_name[i], np.around(weight[i]*100, 2), *array_interweave(result1[i], result2[i]).tolist()))\n",
    "\n",
    "    row_format = \"{:>20}\" + \"{:>12}\" + \"{:>10}|{:>5}\"*5\n",
    "    logger_func(row_format.format(*['-']*12))\n",
    "    \n",
    "    mean_result1 = np.around(np.mean(np.array([result_label1.accuracy, result_label1.mean_accuracy, result_label1.precision, result_label1.recall, result_label1.f1_score]), axis=1)*100, 2)\n",
    "    \n",
    "    mean_result2 = np.around(np.mean(np.array([result_label2.accuracy, result_label2.mean_accuracy, result_label2.precision, result_label2.recall, result_label2.f1_score]), axis=1)*100, 2)\n",
    "    \n",
    "    row_format = \"{:>20}\" + \"{:>12}\"\n",
    "    for i in range(5):\n",
    "        if mean_result1[i] > mean_result2[i]:\n",
    "            row_format += color + \"{:>10}\" + COLOR.END +\"|{:>5}\"\n",
    "        elif mean_result1[i] < mean_result2[i]:\n",
    "            row_format += \"{:>10}|\" + color + \"{:>5}\" + COLOR.END\n",
    "        else:\n",
    "            row_format += color + \"{:>10}\" + COLOR.END + \"|\" + color + \"{:>5}\" + COLOR.END\n",
    "\n",
    "    logger_func(row_format.format(\n",
    "        'mean',\n",
    "        '-',\n",
    "        round(np.mean(result_label1.accuracy)*100, 2),\n",
    "        round(np.mean(result_label2.accuracy)*100, 2),\n",
    "        round(np.mean(result_label1.mean_accuracy)*100, 2),\n",
    "        round(np.mean(result_label2.mean_accuracy)*100, 2),\n",
    "        round(np.mean(result_label1.precision)*100, 2),\n",
    "        round(np.mean(result_label2.precision)*100, 2),\n",
    "        round(np.mean(result_label1.recall)*100, 2),\n",
    "        round(np.mean(result_label2.recall)*100, 2),\n",
    "        round(np.mean(result_label1.f1_score)*100, 2),\n",
    "        round(np.mean(result_label2.f1_score)*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(config, datamanager, logger_func):\n",
    "    cfg_trainer = config['trainer_colab'] if config['colab'] == True else config['trainer']\n",
    "\n",
    "    use_gpu = cfg_trainer['n_gpu'] > 0 and torch.cuda.is_available()\n",
    "    device = torch.device('cuda:0' if use_gpu else 'cpu')\n",
    "    map_location = \"cuda:0\" if use_gpu else torch.device('cpu')\n",
    "    \n",
    "    model, _ = build_model(config, num_classes=len(datamanager.datasource.get_attribute()))\n",
    "\n",
    "    logger_func('Loading checkpoint: {} ...'.format(config['resume']))\n",
    "    checkpoint = torch.load(config['resume'], map_location=map_location)\n",
    "\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    with tqdm(total=len(datamanager.get_dataloader('test'))) as epoch_pbar:\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, _labels) in enumerate(datamanager.get_dataloader('test')):\n",
    "                data, _labels = data.to(device), _labels.to(device)\n",
    "\n",
    "                out = model(data)\n",
    "\n",
    "                _preds = torch.sigmoid(out)\n",
    "                preds.append(_preds)\n",
    "                labels.append(_labels)\n",
    "                epoch_pbar.update(1)\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    preds = preds.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    result_label, result_instance = recognition_metrics(labels, preds)\n",
    "\n",
    "    return result_label, result_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config1 = \"config/baseline_peta.yml\"\n",
    "config2 = \"config/episode_peta.yml\"\n",
    "\n",
    "resume1 = \"/content/drive/Shared drives/REID/HIEN/Models/OSNet_Person_Attribute_Refactor/checkpoints/0731_232453/model_best_accuracy.pth\"\n",
    "resume2 = \"/content/drive/Shared drives/REID/HIEN/Models/person_attribute_recognition/checkpoints/0809_231322/model_best_accuracy.pth\"\n",
    "\n",
    "config1 = read_config(config1)\n",
    "config1.update({'resume': resume1})\n",
    "config1.update({'colab': True})\n",
    "\n",
    "config2 = read_config(config2)\n",
    "config2.update({'resume': resume2})\n",
    "config2.update({'colab': True})\n",
    "\n",
    "datamanager1 = DataManger_Epoch(config1['data'])\n",
    "datamanager2 = DataManger_Episode(config2['data'])\n",
    "\n",
    "weight = datamanager1.datasource.get_weight('train')\n",
    "\n",
    "# model1\n",
    "result_label1, result_instance1 = test(config1, datamanager1, print)\n",
    "\n",
    "# model 2\n",
    "result_label2, result_instance2 = test(config2, datamanager2, print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compare_class_based(print, datamanager1.datasource.get_attribute(), weight, result_label1, result_label2, COLOR.BLUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight = datamanager1.datasource.get_weight('train')\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.multinomial import Multinomial\n",
    "m = Multinomial(32, torch.exp(1-torch.tensor(weight)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_name = datamanager1.datasource.get_attribute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "selected_attribute = random.sample(attribute_name, 8)\n",
    "print(selected_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Multinomial(probs=torch.exp(1-torch.tensor(weight))).sample(torch.Size([8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multinomial(torch.exp(1-torch.tensor(weight)), 8, replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import defaultdict\n",
    "result = defaultdict(int)\n",
    "for _ in range(100000):\n",
    "    temp = torch.multinomial(torch.exp(1-torch.tensor(weight)), 8, replacement=True)\n",
    "    for key, value in collections.Counter(temp.numpy()).items():\n",
    "        result[key] += value\n",
    "print(list(collections.OrderedDict(sorted(result.items())).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}